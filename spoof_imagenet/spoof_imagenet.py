# -*- coding: utf-8 -*-import osimport timeimport argparseimport numpy as npimport mathimport copyfrom pprint import pprintfrom initial_break import *from PIL import Imageimport matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import gradimport torchvisionfrom torchvision import models, datasets, transformsfrom torch.utils.data import DataLoaderimport torch.utils.data as Dataprint(torch.__version__, torchvision.__version__)from folder import ImageFolderfrom model import resnet20, resnet50, _weights_initfrom model_imagenet import resnet18from utils import *########### config #########parser = argparse.ArgumentParser(description='spoof PoL - spoof')parser.add_argument('--iter', type=int, default=20300, help="Wt iterations")parser.add_argument('--dataset', type=str, default="imagenet10", help="CIFAR10 or CIFAR100")parser.add_argument('--model', type=str, default="resnet18", help="resnet20 or resnet50")parser.add_argument('--lr', type=float, default=0.01, help = "lr")parser.add_argument('--t', type=int, default=20, help = "t from W0' to Wn")parser.add_argument('--k', type=int, default=100, help = "equal to freq")parser.add_argument('--batchsize', type=int, default=128, help = "equal to freq")parser.add_argument('--retry', type=int, default=7, help = "retry times")parser.add_argument('--gd', type=float, default=3, help = "grad diff")parser.add_argument('--nd', type=int, default=5, help = "noise diff")parser.add_argument('--round', type=int, default=1, help="dlg training rounds")parser.add_argument('--verify', type=int, default=1, help="verify or not")parser.add_argument('--seed', type=int, default=777, help="lucky number")args = parser.parse_args()dif_k = 0experiment_times = 1different_t_settings = [10]total_time_comsume =[[] for i in different_t_settings]total_diff_dis = [[] for i in different_t_settings]for diff_k in different_t_settings:    args.t = diff_k    for test_times in range(experiment_times):        diff = [[], [], [], []]        for k in args.__dict__:            print(k + ": " + str(args.__dict__[k]))        init_threshold = 0.01        order = ['1', '2', 'inf', 'cos']        threshold = [1000, 10, 0.1, 0.01]        ########### config #########        if args.seed > 0:            seed = args.seed            torch.manual_seed(seed)            torch.cuda.manual_seed(seed)        device = "cpu"        if torch.cuda.is_available():            device = "cuda:0"        print("Running on %s" % device)        net = resnet18().to(device)        net.apply(_weights_init)        cur_param = list((_.detach().clone() for _ in net.parameters()))        print("model have {} paramerters in total".format(sum(x.numel() for x in net.parameters())))        criterion = nn.CrossEntropyLoss().to(device)        apply_transform = transforms.Compose([                        transforms.Scale(128),                        transforms.CenterCrop(128),                        transforms.ToTensor(),                        #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),                        ])        train_dataset = ImageFolder(                    root="./data/train/cut_train",                    transform=apply_transform,                    classes_idx=(0, 10))        train_loader = Data.DataLoader(            dataset=train_dataset,            shuffle=True,            batch_size=args.batchsize        )        print("--------- load checkpoint!----------")        net_load = resnet18().to(device)        state = torch.load("proof/{}/model_step_0".format(args.dataset), map_location={'cuda:0':'cuda:0'})        net_load.load_state_dict(state['net'])        # before train parameter        bt_param = list((_.detach().clone() for _ in net_load.parameters()))        state = torch.load("proof/{}/model_step_".format(args.dataset) + str(args.iter), map_location={'cuda:0':'cuda:0'})        net_load.load_state_dict(state['net'])        # after train parameter        at_param = list((_.detach().clone() for _ in net_load.parameters()))        net.load_state_dict(state['net'])        t = net.state_dict()        p_list = []        if args.model == 'resnet18':            for name, param in net.named_parameters():                if 'fc' in name or 'conv' in name or 'linear' in name:                    if 'weight' in name:                        x = generate_random(param).reshape_as(param)                        # x = generate_uniform(param).reshape_as(param)                        t[name].copy_(x)                        param.data.copy_(x)                        p_list.append(check(param))        elif args.model =='resnet18':            for name, param in net.named_parameters():                if len(param.shape) == 4:                    x = generate_random(param).reshape_as(param)                    # x = generate_uniform(param).reshape_as(param)                    t[name].copy_(x)                    param.data.copy_(x)                    p_list.append(check(param))                elif 'weight' in name and 'fc' in name:                    x = generate_random(param).reshape_as(param)                    # x = generate_uniform(param).reshape_as(param)                    t[name].copy_(x)                    param.data.copy_(x)                    p_list.append(check(param))                elif 'bias' in name and ('fc' in name or 'linear' in name):                    weight = net.state_dict()[name.replace('bias', 'weight')]                    x = generate_random_bias(param, weight).reshape_as(param)                    t[name].copy_(x)                    param.data.copy_(x)                    p_list.append(check_bias(param, weight))        cur_param = list((_.detach().clone() for _ in net.parameters()))        dis = 0        for gx, gy in zip(at_param, bt_param):            dis += ((gx - gy) ** 2).sum()        dis_W0 = 0        for gx, gy in zip(bt_param, cur_param):            dis_W0 += ((gx - gy) ** 2).sum()        dis_cur = 0        for gx, gy in zip(at_param, cur_param):            dis_cur += ((gx - gy) ** 2).sum()        # print("sum distance: ", count)        # print("avg distance: ", count / args.iter)        print("|| Wt-W0 || distance: ", math.sqrt(dis.item()))        print("|| W0-W0'|| distance: ", math.sqrt(dis_W0.item()))        print("|| Wt-W0'|| distance: ", math.sqrt(dis_cur.item()))        print("")        print("--------- verify init!----------")        # verify init        save_path = "spoof/{}/dataset".format(args.dataset)        folder = os.path.exists(save_path)        if not folder:            os.makedirs(save_path)        state = {'net': net.state_dict()}        torch.save(state, "spoof/{}/model_step_0".format(args.dataset))        f = open("spoof/{}/dataset.txt".format(args.dataset),"w")        print("--------- spoof start!----------")        img_idx=0        tt_sum = 0        original_weight = copy.deepcopy(net.state_dict())        step_weight = copy.deepcopy(net.state_dict())        at_weight = copy.deepcopy(net_load.state_dict())        for key in at_weight.keys():            tmp = torch.sub(at_weight[key], original_weight[key])            step_weight[key] = torch.div(tmp, args.t)        cur_param = list(_.detach().clone() for _ in net.parameters())        k_step_dy_dx = list(torch.div(torch.sub(cur, at), (args.t)) for (at, cur) in zip(at_param, cur_param))        valid_count = 0        for i in range(args.t):            print("step:", i+1)            cur_param = list((_.detach().clone() for _ in net.parameters()))            target_param = list((torch.add(-s, cur) for (s, cur) in zip(k_step_dy_dx, cur_param)))            original_weight = copy.deepcopy(net.state_dict())            # target_param = list((_.detach().clone() for _ in net.parameters()))            optimizer_net = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)            for j in range(args.k):                start_time = time.time()                tmp_param = list((_.detach().clone() for _ in net.parameters()))                step_dy_dx = list(torch.sub(tmp, cur) for (cur, tmp) in zip(cur_param, tmp_param))                zero_dy_dx = list(torch.zeros(cur.shape).to(device) for cur in cur_param) # equal to zeros                # generate dummy data and label                for k in range(args.retry):                    dummy_data = torch.zeros(args.batchsize, 3, 128, 128).to(device).requires_grad_(True)                    optimizer = torch.optim.LBFGS([dummy_data, ])                    for ix, data in enumerate(train_loader):                        img, label = data                        dummy_label = label.to(device)                        break                    for iters in range(args.round):                        def closure():                            optimizer.zero_grad()                            x = torch.clamp(dummy_data + img.to(device), 0, 1)                            dummy_pred = net(x)                            dummy_loss = criterion(dummy_pred, dummy_label)                            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True, retain_graph=True)                            grad_diff = 0                            for gx, gy in zip(dummy_dy_dx, zero_dy_dx):                                grad_diff += ((gx - gy) ** 2).sum()                            grad_diff += 1*(dummy_data ** 2).sum()                            grad_diff.backward()                            return grad_diff                        optimizer.step(closure)                        if (iters+1) % 1 == 0:                            current_loss = closure()                            d2r = (dummy_data** 2).sum().item()                            d2grad = current_loss.item() - d2r                            print("step: %d iters: %d loss: %.8f" % (j+1, iters+1, current_loss.item()))                            print(" d2(r, 0):", d2r)                            print(" d2(w, w):", d2grad)                    if (d2grad < args.gd and d2r < args.nd):                        break                    print("retry!!")                # show fake data                fake_img = torch.clamp(dummy_data.cpu() + img.cpu(), 0, 1)                for k in range(args.batchsize):                    saveImg(fake_img[k], "spoof/{}/dataset/%d.png".format(args.dataset) % img_idx)                    f.write("spoof/{}/dataset/%d.png %d\n".format(args.dataset) % (img_idx, dummy_label[k].item()))                    img_idx += 1                step_time = time.time() - start_time                print("step time:", step_time)                tt_sum += step_time                if args.verify:                    flag = 0                    x = fake_img.detach().to(device)                    y = dummy_label.detach()                    optimizer_net.zero_grad()                    pred = net(x)                    loss = criterion(pred, y)                    loss.backward()                    optimizer_net.step()            dummy_param = list((_.detach().clone() for _ in net.parameters()))            dis = 0            for gx, gy in zip(dummy_param, cur_param):                dis += ((gx - gy) ** 2).sum()            print("dum - cur:", dis.item())            dis = 0            for gx, gy in zip(target_param, cur_param):                dis += ((gx - gy) ** 2).sum()            print("tar - cur:", dis.item())            dis = 0            for gx, gy in zip(dummy_param, target_param):                dis += ((gx - gy) ** 2).sum()            print("dum - tar:", dis.item())            dist_list = [[] for i in range(len(order))]            res = parameter_distance(target_param, cur_param, order=order)            for j in range(len(order)):                dist_list[j].append(res[j])            dist_list = np.array(dist_list)            for k in range(len(order)):                print(f"{order[k]} : {np.average(dist_list[k])}")            dist_list = [[] for i in range(len(order))]            res = parameter_distance(target_param, dummy_param, order=order)            for j in range(len(order)):                dist_list[j].append(res[j])            dist_list = np.array(dist_list)            for k in range(len(order)):                print(f"Distance metric: {order[k]} || threshold: {threshold[k]}")                print(f"Average distance: {np.average(dist_list[k])}")                above_threshold = np.sum(dist_list[k] > threshold[k])                if above_threshold == 0:                    print("None of the steps is above the threshold, the proof-of-learning is valid.")                else:                    print(f"{above_threshold} / {dist_list[k].shape[0]} "                                f"({100 * np.average(dist_list[k] > threshold[k])}%) "                                f"of the steps are above the threshold, the proof-of-learning is invalid.")                    flag = 1            if flag == 0:                valid_count += 1            print("=> valid rate: (%d/ %d), total: %d" % (valid_count, i+1, args.t))            print("")            diff = np.concatenate([diff, dist_list], 1)            new_weight = copy.deepcopy(net.state_dict())            for key in original_weight.keys():                 new_weight[key] = torch.add(original_weight[key], step_weight[key])            net.load_state_dict(new_weight)            state = {'net': net.state_dict()}            torch.save(state, "spoof/{}/model_step_%d".format(args.dataset) % (args.k * (i+1)))            print("total_time: ",tt_sum)        print("--------- conclusion ----------")        different_order = {order[0]:[],order[1]:[], order[2]:[],order[3]:[]}        for k, name in enumerate(order):            print("order:", name)            print("distance:", np.mean(diff[k]), "  min: ", np.min(diff[k]), "  max: ", np.max(diff[k]))            different_order[name]={"mean":np.mean(diff[k]),"min": np.min(diff[k]), "max": np.max(diff[k])}        total_diff_dis[dif_k].append(different_order)        total_time_comsume[dif_k].append(tt_sum)    dif_k+=1final_result=[{order[0]:0, order[1]:0, order[2]:0, order[3]:0} for i in range(len(different_t_settings))]for i in range(len(different_t_settings)):    for name in order:        mean_tmp = []        min_tmp = []        max_tmp = []        for j in range(experiment_times):            mean_tmp.append(total_diff_dis[i][j][name]['mean'])            min_tmp.append(total_diff_dis[i][j][name]['min'])            max_tmp.append(total_diff_dis[i][j][name]['max'])        final_result[i][name] = {"mean": np.mean(mean_tmp), "min": np.min(min_tmp), "max": np.max(max_tmp)}    print("current t is ", different_t_settings[i], ",   final_eps: ", final_result[i])    print("current t is ", different_t_settings[i], "total_time_comsume: ", total_time_comsume[i])